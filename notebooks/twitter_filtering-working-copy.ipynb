{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import sys, os, glob\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import HTML\n",
    "import xml.etree.ElementTree as ET\n",
    "try:\n",
    "    tree = ET.parse(os.environ['HADOOP_CONF_DIR'] + '/yarn-site.xml')\n",
    "except IOError:\n",
    "    raise IOError(\"Can't find the yarn configuration -- is HADOOP_CONF_DIR set?\")\n",
    "root = tree.getroot()\n",
    "yarn_web_app = root.findall(\"./property[name='yarn.resourcemanager.webapp.address']\")[0].find('value').text\n",
    "yarn_web_app_string = \"If this works successfully, you can check the <a target='_blank' href='http://{yarn_web_app}'>YARN application scheduler</a> and you should see your app listed there. Clicking on the 'Application Master' link will bring up the familiar Spark Web UI. \"\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and launch the Spark runtime\n",
    "\n",
    "Remember from the previous notebook that we have a saved configuration in `./spark_config/` -- so all we need to do is set the `SPARK_CONF_DIR` environment variable and our default configuration will be used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify the configuration directory\n",
    "os.environ['SPARK_CONF_DIR'] = os.path.realpath('../../spark_workshop/notebooks/twitter_dataframes/spark_config')\n",
    "\n",
    "# how many cores do we have for the driver\n",
    "ncores = int(os.environ.get('LSB_DJOB_NUMPROC', 1)) \n",
    "\n",
    "# here we set the memory we want spark to use for the driver JVM\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '%dG'%(ncores*0.7)\n",
    "\n",
    "# we have to tell spark which python executable we are using\n",
    "os.environ['PYSPARK_PYTHON'] = subprocess.check_output('which python', shell=True).rstrip()\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "sc = SparkContext(master='yarn-client', conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Using `DataFrame`s, our entry point into the Spark universe is the `SQLContext` or the equivalent `HiveContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, HiveContext\n",
    "\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the data off the disk, but only for the last three months of the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 ms, sys: 3 ms, total: 10 ms\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data = hc.read.parquet('/user/roskarr/twitter/2014_1*')\n",
    "data = hc.read.parquet('/user/roskarr/twitter/2014_12*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` is now a `DataFrame` object, which is essentially a collection of `Row` objects. Each `Row` object contains data for whatever columns are defined in the `DataFrame`. Here is a critical difference between `DataFrames` and `RDD`s: each column has an associated data type. While in dealing with an `RDD` we relied on Python to convert types, here each column has a specified data type. This means that:\n",
    "\n",
    "a) we have to be a bit more careful about what we are doing and \n",
    "\n",
    "b) the execution engine can optimize our calculations because the data is no longer a black box. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and types for `DataFrame` API\n",
    "\n",
    "The next few cells are a bit lengthy and complicated so you can just treat them as black boxes for now. First, we just import some necessary libraries and classes, then a series of functions are defined and executed on the `data` to give us a `DataFrame` of hashtags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import Row, Window\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType, NullType, LongType, StructField, StructType, DateType, DataType, DateConverter, DatetimeConverter, TimestampType, BooleanType\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keys(path='../docs/keywords.txt'):\n",
    "    keys = np.genfromtxt(path, dtype=\"|S20\", delimiter='#', autostrip=True)\n",
    "    return keys.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "keys = get_keys()\n",
    "print(type(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['isis', 'terrorism', 'arab', 'spring', 'attack', 'god', 'christian', 'allah', 'islam', 'syria', 'refugees', 'migrants', 'africa', 'italy', 'ethiopia', 'asylum', 'unhcr', 'immigration', 'foreigners', 'crowded', 'ebola', 'guinea', 'sierra', 'leone', 'liberia', 'virus', 'epidemic', 'vaccine', 'who', 'influenza', 'flu', 'birds', 'swine', 'pig', 'bitcoin', 'rosetta', 'comet', 'higgs', 'climate', 'doomsday', 'maya', 'curiosity', 'sandy', 'hurricane', 'black', 'white', 'mandela', 'nelson', 'left', 'right', 'mh17', 'mh370', 'ukraine', 'crimea', 'russia', 'snowden', 'nsa', 'obama', 'putin', 'pope', 'unemployment', 'boston', 'marathon', 'london', 'europe', 'usa', 'philippines', 'sochi', 'olympics', 'geneva', 'apple', 'linux', 'PC', 'google', 'iphone', 'galaxy', 'watch', 'facebook', 'twitter', 'whatsapp', 'vegan', 'gluten', 'vegetarian', 'meat', 'pasta', 'banana', 'family', 'divorce', 'marriage', 'wedding', 'holidays', 'homework', 'television', 'coffee', 'tea', 'school', 'work', 'teacher', 'sports', 'jogging']\n"
     ]
    }
   ],
   "source": [
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions to extract hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the date format to YYYY-MM-DD HH:MM:SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_date_string = func.udf(lambda date_string: \\\n",
    "                               datetime.date.strftime(datetime.datetime.strptime(date_string, \\\n",
    "                                                                                 '%a %b %d %H:%M:%S +0000 %Y'),\\\n",
    "                                                      '%Y-%m-%d %H:%M:%S'), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the date string to a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime_udf = func.udf(lambda date_string: datetime.strptime(date_string, '%a %b %d %H:%M:%S +0000 %Y'), DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the hashtags from a hashtag array and convert them all to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_text_udf = func.udf(lambda row: [r.text.lower() for r in row], returnType=ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tweet text string to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_udf_lower = func.udf(lambda row: row.lower(), returnType=StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_udf_split = func.udf(lambda row: row.split(), returnType=ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the year of creation of the tweet and the day of creation to a unique date ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_date = (lambda col: func.dayofyear(col) + func.year(col)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_udf_key = func.udf(lambda row: len([val for val in row if val in keys]) > 0, returnType=BooleanType())\n",
    "# key_udf_key = func.udf(lambda row: keys[0] in row, returnType=BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the functions to the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of partitions for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "Npartitions = sc.defaultParallelism*200\n",
    "print(Npartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the hashtag dataframe with all the tweets with more than one hashtag, extract the hashtags and also extract the creation date and convert it to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: int, text: array<string>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep the tweets with at least one hashtag\n",
    "hashtag_df = (data.select('created_at', 'text')\n",
    "              .filter(func.length('text') > 0)\n",
    "              .withColumn('created_at', convert_date_string('created_at'))\n",
    "              .withColumn('created_at', new_date('created_at'))\n",
    "              .withColumn('text', key_udf_lower('text'))\n",
    "              .withColumn('text', key_udf_split('text'))\n",
    "              .repartition(Npartitions)\n",
    "              .filter(key_udf_key('text')))\n",
    "#               .repartition(Npartitions))\n",
    "hashtag_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|created_at|text                                                                                                                                                                     |\n",
      "+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2014338   |[rt, @lashanekayla:, once, you, stop, chasing, the, wrong, things,, the, right, ones, catch, you..]                                                                      |\n",
      "|2014338   |[rt, @rbcreamer:, yoho, immigration, bill, cements, gop, position, favoring, mass, deportation, of, immigrants., #immigrantaction]                                       |\n",
      "|2014335   |[rt, @binkylondon:, just, 3, hours, left, to, get, 25%, off, everything, at, http://t.co/h8gvelkkhk, use, code, cyber25, at, checkout, #cybermonday, http://t.câ€¦]        |\n",
      "|2014337   |[rt, @apeppa:, por, mim, o, whatsapp, pode, colocar, o, visto, de, azul, neon, piscando, como, se, fosse, em, las, vegas, q, eu, sÃ³, vou, responder, quando, eu, quiserâ€¦]|\n",
      "|2014341   |[i, have, been, on, twitter, for, 3, years, and, 7, months, (since, 7, may, 2011)., and, you?, http://t.co/v5c3sythl5]                                                   |\n",
      "|2014336   |[6x, glico, pocky, matcha, green, tea, flavour, biscuit, stick, confectionery, 39g/thailand, http://t.co/vxtujitmmn, via, @ebay]                                         |\n",
      "|2014337   |[rt, @curryramyeon:, so, aoa's, the, one, who, received, the, award, for, cnblue;, there, goes, your, \"no, attendance,, no, award\", reasoning.]                          |\n",
      "|2014336   |[rt, @sportscenter:, \"i, think, i, got, my, swagger, back!\", icymi:, these, high, school, football, players, are, all, kinds, of, fired, up, to, be, on, tv., http://â€¦]  |\n",
      "|2014339   |[artie, lange, responds, to, a, parody, tacs, account, on, twitter, thinking, it', http://t.co/c22kq2cfdz]                                                               |\n",
      "|2014340   |[rt, @roverporn:, triple, black, ðŸ˜ðŸ˜±, http://t.co/1t6g8hvc5k]                                                                                                           |\n",
      "+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashtag_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|created_at|                text|\n",
      "+----------+--------------------+\n",
      "|   2014335|[rt, @mobi4all_en...|\n",
      "|   2014337|[apple, :, un, br...|\n",
      "|   2014338|[apple, a, suppri...|\n",
      "|   2014337|[apple/android?, ...|\n",
      "|   2014338|[kolumuzda, hiÃ§, ...|\n",
      "|   2014337|[rt, @appieoffici...|\n",
      "|   2014336|[apple, macbook, ...|\n",
      "|   2014339|[rt, @nrinsyfqh_:...|\n",
      "|   2014335|[google, chromebo...|\n",
      "|   2014335|[#apple, #iphone6...|\n",
      "+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_func = func.udf(lambda row: len([val for val in row if val == 'apple']) > 0, returnType=BooleanType())\n",
    "count_func = func.udf(lambda row: 1, returnType=IntegerType())\n",
    "\n",
    "temp_dt = (hashtag_df.repartition(Npartitions)\n",
    "           .filter(filter_func('text'))\n",
    "           )\n",
    "temp_dt.show(10)\n",
    "group_date = temp_dt.groupBy('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|created_at|text                                                                                                                                                           |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2014335   |[rt, @mobi4all_en:, five, reasons, why, you, should, buy, apple, iphone, 6, plus, -, http://t.co/dn6hvtore1, #iphone6plus, http://t.co/e7tayrarni]             |\n",
      "|2014337   |[apple, :, un, brevet, pour, en, finir, avec, les, Ã©crans, d'iphone, brisÃ©s, ?, http://t.co/u7lr1hbdkp, via, zdnet]                                            |\n",
      "|2014338   |[apple, a, supprimÃ©, des, chansons, d'ipods, sans, en, avertir, leurs, utilisateurs, -, rts.ch, -, info, -, sciences-tech., http://t.co/ap1n0e7v4x]            |\n",
      "|2014337   |[apple/android?, â€”, apple, http://t.co/oe1cdggkho]                                                                                                             |\n",
      "|2014338   |[kolumuzda, hiÃ§, de, fena, gÃ¶rÃ¼nmeyecek, 12, apple, watch, kayÄ±ÅŸÄ±, http://t.co/yefmgszxeo, http://t.co/guhkapl0dx]                                             |\n",
      "|2014336   |[apple, macbook, a1181, euã‚­ãƒ¼ãƒœãƒ¼ãƒ‰, ç™½, [junkãƒ»ç¾Žå“], 140, http://t.co/pkxkxdrvel]                                                                                    |\n",
      "|2014337   |[rt, @appieofficiel:, apple, glass, -, the, future, is, here, ðŸ“±, http://t.co/izsxmmo08j]                                                                      |\n",
      "|2014339   |[rt, @nrinsyfqh_:, i, told, ya, my, mum, abt, my, apple, missing, and, this, morning, she, bought, fr, me, ðŸ˜‚ðŸ˜‚]                                               |\n",
      "|2014335   |[google, chromebooks, trump, apple, ipads, in, schools,, says, idc:, while, some, might, contend, google's, dominance, in, cert..., http://t.co/9pdn9x4ysb]    |\n",
      "|2014336   |[rt, @nsicargo:, http://t.co/uzirqd221p, envia, con, nsi, cargo, http://t.co/lddgboquhb, apple, iphone, 4s, -, 16gb, -, black, (verizon), sm..., http://t.co/â€¦]|\n",
      "+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_dt.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|created_at|count|\n",
      "+----------+-----+\n",
      "|   2014335| 1606|\n",
      "|   2014336| 2109|\n",
      "|   2014337| 1984|\n",
      "|   2014338| 2163|\n",
      "|   2014339| 2001|\n",
      "|   2014340| 1773|\n",
      "|   2014341|  206|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_df = group_date.count()\n",
    "group_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group_df.show(10)\n",
    "# group_df.coalesce(1).write.format('com.databricks.spark.csv').save('../data/obama.csv')\n",
    "# group_df.rdd.map(lambda x: \",\".join(map(str, x))).coalesce(1).saveAsTextFile('/cluster/home03/phys/anweigel/spuriousrelations/data/obama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(created_at=2014335, count=1471),\n",
       " Row(created_at=2014336, count=1970),\n",
       " Row(created_at=2014337, count=1660),\n",
       " Row(created_at=2014338, count=2237),\n",
       " Row(created_at=2014339, count=1883),\n",
       " Row(created_at=2014340, count=1263),\n",
       " Row(created_at=2014341, count=164)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group_df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014335,1471',\n",
       " '2014336,1970',\n",
       " '2014337,1660',\n",
       " '2014338,2237',\n",
       " '2014339,1883',\n",
       " '2014340,1263',\n",
       " '2014341,164']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group_df.rdd.map(lambda x: \",\".join(map(str, x))).collect().saveAsTextFile('/cluster/home03/phys/anweigel/spuriousrelations/data/obama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brrr = np.array(group_df.rdd.map(lambda x: \",\".join(map(str, x))).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('/cluster/home03/phys/anweigel/spuriousrelations/data/apple.csv', brrr, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group_df.rdd.map(lambda x: \",\".join(map(str, x))).foreach(print)  #map(lambda x: \",\".join(map(str, x))).coalesce(1)#.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|created_at|count|\n",
      "+----------+-----+\n",
      "|   2014335| 1471|\n",
      "+----------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    filter_func = func.udf(lambda row: len([val for val in row if val == key]) > 0, returnType=BooleanType())\n",
    "    temp_dt = (hashtag_df.repartition(Npartitions)\n",
    "               .filter(filter_func('text'))\n",
    "              )\n",
    "    group_date = temp_dt.groupBy('created_at')\n",
    "    group_df = group_date.count()\n",
    "    arr = np.array(group_df.rdd.map(lambda x: \",\".join(map(str, x))).collect())\n",
    "    np.savetxt('/cluster/home03/phys/anweigel/spuriousrelations/data/' + key + '.csv',\n",
    "               arr, fmt='%s')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tweet text to lowercase, split it into a word array and add this into a new column called test. Then check if any of the elements of keys is in this list and add this information in the column intext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testfunc(row, keys):\n",
    "    result_tmp = [True for val in keys]\n",
    "#     result_tmp = np.array(result_tmp, dtype=np.int)    \n",
    "    return result_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = ['@tom_wilso:', 'rt']\n",
    "# key_udf_key = func.udf(lambda row: len([val for val in keys if val in row]) > 0, returnType=BooleanType())\n",
    "key_udf_global3 = func.udf(lambda row: [int(val in row) for val in keys], returnType=ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_df = hashtag_df.withColumn('test', key_udf_lower('text'))\n",
    "hashtag_df = hashtag_df.withColumn('test', key_udf_split('test'))\n",
    "hashtag_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = ['@oridiva:', 'rt']\n",
    "# key_udf_key = func.udf(lambda row: len([val for val in keys if val in row]) > 0, returnType=BooleanType())\n",
    "key_udf_global3 = func.udf(lambda row: [int(val in row) for val in keys], returnType=ArrayType(IntegerType()))\n",
    "key_udf_global4 = func.udf(lambda row: int('@oridiva:' in row), returnType=IntegerType())\n",
    "key_udf_global5 = func.udf(lambda row: int('rt' in row), returnType=IntegerType())\n",
    "\n",
    "hashtag_df5 = (hashtag_df.withColumn('@oridiva:', key_udf_global4('test'))\n",
    "                         .withColumn('rt', key_udf_global5('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_df5.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_df2 = hashtag_df.withColumn('intext', key_udf_global3('test'))\n",
    "hashtag_df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the day ID of the tweet and add it to the column new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_df3 = hashtag_df5.withColumn('new_date', new_date('created_at'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hashtag_df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_df4 = hashtag_df3.select('new_date', '@oridiva:', 'rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_df4.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtag_df4_group = hashtag_df4.groupBy('new_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_df4_group.sum('@oridiva:').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Questions:\n",
    "* What does the text method do?\n",
    "* Why does the new_date function work even though it is not an UDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /user/roskarr/twitter/: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls /user/roskarr/twitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LSB_MAX_NUM_PROCESSORS': '200', 'LSF_EAUTH_AUX_PASS': 'yes', 'TMOUT': '86400', '_JAVA_OPTIONS': '-Xmx1024m -Xms256m -XX:ParallelGCThreads=1', 'BSUB_BLOCK_EXEC_HOST': '', 'SHELL': '/bin/bash', 'LSB_DJOB_HB_INTERVAL': '62', 'LSB_UNIXGROUP_INT': 'phys', 'HISTSIZE': '1000', 'LS_EXEC_T': 'START', 'MANPATH': '/cluster/apps/java/jdk1.7.0_03/man:/cluster/apps/modules/share/man:/cluster/apps/lsf/8.0/man:', 'LSB_EEXEC_REAL_GID': '', 'JAVA_HOME': '/cluster/apps/java/jdk1.7.0_03', 'JPY_PARENT_PID': '7581', 'LSF_INVOKE_CMD': 'bsub', 'HADOOP_CONF_DIR': '/cluster/apps/hadoop/hadoop-2.6.0/etc/hadoop', 'LSB_JOBINDEX': '0', 'LS_JOBPID': '7424', 'LSB_JOBID': '16275679', 'HOSTNAME': 'a6264', 'MCR_CACHE_ROOT': '/scratch/matlab_mcr_cache_root_69413', 'LSB_JOBRES_PID': '7424', 'MAIL': '/var/spool/mail/anweigel', 'LS_COLORS': 'rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lz=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.bz=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.rar=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:', 'C_INCLUDE_PATH': '/cluster/apps/java/jdk1.7.0_03/include:/cluster/apps/java/jdk1.7.0_03/include/linux', 'LSB_TRAPSIGS': 'trap # 15 10 12 2 1', 'LSFUSER': 'anweigel', 'LSF_EAUTH_SERVER': 'mbatchd@brutus', 'LESSOPEN': '||/usr/bin/lesspipe.sh %s', 'LSF_BINDIR': '/cluster/apps/lsf/8.0/linux2.6-glibc2.3-x86_64/bin', 'CPLUS_INCLUDE_PATH': '/cluster/apps/java/jdk1.7.0_03/include:/cluster/apps/java/jdk1.7.0_03/include/linux', 'CPATH': '/cluster/apps/java/jdk1.7.0_03/include:/cluster/apps/java/jdk1.7.0_03/include/linux', 'USER': 'anweigel', 'LSB_EEXEC_REAL_UID': '', 'LSB_DJOB_NUMPROC': '200', 'LSB_EXEC_HOSTTYPE': 'X86_64', 'HADOOP_PREFIX': '/cluster/apps/hadoop/hadoop-2.6.0', 'SHLVL': '3', 'LSB_MCPU_HOSTS': 'a6264 16 a6220 16 a6368 16 a6327 16 a6272 16 a6325 16 a6333 16 a6286 16 a6211 16 a6354 16 a6344 16 a6297 16 a6278 8 ', 'DISPLAY': 'brutus2:18.0', 'HADOOP_YARN_HOME': '/cluster/apps/hadoop/hadoop-2.6.0', 'LSB_JOBFILENAME': '/cluster/home03/phys/anweigel/.lsbatch/1461414232.16275679', 'MODULESHOME': '/cluster/apps/modules', 'LSB_BIND_JOB': 'BALANCE', 'GIT_PAGER': 'cat', 'INCLUDE': '/cluster/apps/java/jdk1.7.0_03/include:/cluster/apps/java/jdk1.7.0_03/include/linux', 'TMPDIR': '/scratch/16275679.tmpdir', 'MODULEPATH': '/cluster/apps/modules/modulefiles', 'SSH_CONNECTION': '129.132.211.237 56295 129.132.8.62 22', 'OMP_NUM_THREADS': '1', 'LSB_JOBEXIT_STAT': '0', '_LMFILES_': '/cluster/apps/modules/modulefiles/modules:/cluster/apps/modules/modulefiles/spark/1.3.1:/cluster/apps/modules/modulefiles/java/1.7.0_03:/cluster/apps/modules/modulefiles/hadoop/2.6.0', 'PAGER': 'cat', 'HOME': '/cluster/home03/phys/anweigel', 'LD_LIBRARY_PATH': '/cluster/apps/java/jdk1.7.0_03/jre/lib/amd64/server:/cluster/apps/lsf/8.0/linux2.6-glibc2.3-x86_64/lib', 'LANG': 'en_US.UTF-8', 'LSF_SERVERDIR': '/cluster/apps/lsf/8.0/linux2.6-glibc2.3-x86_64/etc', 'LSB_EXEC_CLUSTER': 'brutus', 'HADOOP_COMMON_HOME': '/cluster/apps/hadoop/hadoop-2.6.0', 'G_BROKEN_FILENAMES': '1', 'LSB_INTERACTIVE': 'Y', 'CONDA_ENV_PATH': '/cluster/apps/spark/miniconda/envs/spark_workshop', '_': '../../spark_workshop/notebooks/start_notebook.py', 'CONDA_DEFAULT_ENV': 'spark_workshop', 'LSB_JOBRES_CALLBACK': '41379@a6264', 'LSB_CHKFILENAME': '/cluster/home03/phys/anweigel/.lsbatch/1461414232.16275679', 'LSB_SUB_HOST': 'brutus2', 'HADOOP_MAPRED_HOME': '/cluster/apps/hadoop/hadoop-2.6.0', 'LSF_LOGDIR': '/cluster/spool/lsf/log', 'LSB_SHMODE': 'y', 'LSF_EAUTH_CLIENT': 'user', 'BASH_FUNC_module()': '() {  eval `/cluster/apps/modules/bin/modulecmd bash $*`\\n}', 'LSB_JOB_EXECUSER': 'anweigel', 'LSB_JOBNAME': 'bash', 'LSB_HOSTS': 'a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6264 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6220 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6368 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6327 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6272 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6325 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6333 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6286 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6211 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6354 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6344 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6297 a6278 a6278 a6278 a6278 a6278 a6278 a6278 a6278', 'SSH_TTY': '/dev/pts/23', 'LSB_EXIT_PRE_ABORT': '99', 'CVS_RSH': 'ssh', 'LSF_ENVDIR': '/cluster/apps/lsf/conf', 'SPARK_HOME': '/cluster/apps/spark/spark-current', 'LSB_BIND_CPU_LIST': '', 'HOSTTYPE': 'X86_64', 'SSH_CLIENT': '129.132.211.237 56295 22', 'LS_SUBCWD': '/cluster/home03/phys/anweigel/', 'LOGNAME': 'anweigel', 'LSF_LIM_API_NTRIES': '1', 'LIBGL_ALWAYS_INDIRECT': '1', 'PATH': '/cluster/apps/hadoop/hadoop-2.6.0/bin:/cluster/apps/hadoop/hadoop-2.6.0/sbin:/cluster/apps/java/jdk1.7.0_03/bin:/cluster/apps/spark/spark-current/bin:/cluster/apps/spark/miniconda/envs/spark_workshop/bin:/cluster/apps/lsf/8.0/linux2.6-glibc2.3-x86_64/etc:/cluster/apps/lsf/8.0/linux2.6-glibc2.3-x86_64/bin:/cluster/apps/modules/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/cluster/home03/phys/anweigel/bin', 'LSB_QUEUE': 'pub.8h', 'TERM': 'xterm-color', 'LSB_DJOB_HOSTFILE': '/cluster/home03/phys/anweigel/.lsbatch/1461414232.16275679.hostfile', 'LSF_LIBDIR': '/cluster/apps/lsf/8.0/linux2.6-glibc2.3-x86_64/lib', 'HADOOP_INSTALL': '/cluster/apps/hadoop/hadoop-2.6.0', 'LSB_SUB_USER': 'anweigel', 'LSB_ACCT_FILE': '/scratch/.1461414232.16275679.acct', 'SBD_KRB5CCNAME_VAL': '', 'LSB_BATCH_JID': '16275679', 'HADOOP_HDFS_HOME': '/cluster/apps/hadoop/hadoop-2.6.0', 'SPARK_CONF_DIR': '/cluster/home03/phys/anweigel/spark_workshop/notebooks/twitter_dataframes/spark_config', 'LSB_DJOB_RU_INTERVAL': '62', 'LSB_SUB_RES_REQ': 'select[mem<70000] order[-ut] span[ptile=16] same[model] rusage[mem=1024,xs=1]', 'SPARK_DRIVER_MEMORY': '140G', 'OLDPWD': '/cluster/home03/phys/anweigel', 'LOADEDMODULES': 'modules:spark/1.3.1:java/1.7.0_03:hadoop/2.6.0', 'CLICOLOR': '1', 'HISTCONTROL': 'ignoredups', 'PYSPARK_PYTHON': '/cluster/apps/spark/miniconda/envs/spark_workshop/bin/python', 'PWD': '/cluster/home03/phys/anweigel/spuriousrelations/notebooks', 'BINARY_TYPE_HPC': ''}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
