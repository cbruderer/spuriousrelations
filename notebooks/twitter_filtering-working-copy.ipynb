{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import sys, os, glob\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import HTML\n",
    "import xml.etree.ElementTree as ET\n",
    "try:\n",
    "    tree = ET.parse(os.environ['HADOOP_CONF_DIR'] + '/yarn-site.xml')\n",
    "except IOError:\n",
    "    raise IOError(\"Can't find the yarn configuration -- is HADOOP_CONF_DIR set?\")\n",
    "root = tree.getroot()\n",
    "yarn_web_app = root.findall(\"./property[name='yarn.resourcemanager.webapp.address']\")[0].find('value').text\n",
    "yarn_web_app_string = \"If this works successfully, you can check the <a target='_blank' href='http://{yarn_web_app}'>YARN application scheduler</a> and you should see your app listed there. Clicking on the 'Application Master' link will bring up the familiar Spark Web UI. \"\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and launch the Spark runtime\n",
    "\n",
    "Remember from the previous notebook that we have a saved configuration in `./spark_config/` -- so all we need to do is set the `SPARK_CONF_DIR` environment variable and our default configuration will be used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify the configuration directory\n",
    "os.environ['SPARK_CONF_DIR'] = os.path.realpath('./spark_config')\n",
    "\n",
    "# how many cores do we have for the driver\n",
    "ncores = int(os.environ.get('LSB_DJOB_NUMPROC', 1)) \n",
    "\n",
    "# here we set the memory we want spark to use for the driver JVM\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '%dG'%(ncores*0.7)\n",
    "\n",
    "# we have to tell spark which python executable we are using\n",
    "os.environ['PYSPARK_PYTHON'] = subprocess.check_output('which python', shell=True).rstrip()\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "sc = SparkContext(master='yarn-client', conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Using `DataFrame`s, our entry point into the Spark universe is the `SQLContext` or the equivalent `HiveContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, HiveContext\n",
    "\n",
    "hc = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the data off the disk, but only for the last three months of the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 ms, sys: 2 ms, total: 17 ms\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = hc.read.parquet('/user/roskarr/twitter/2014_1*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data` is now a `DataFrame` object, which is essentially a collection of `Row` objects. Each `Row` object contains data for whatever columns are defined in the `DataFrame`. Here is a critical difference between `DataFrames` and `RDD`s: each column has an associated data type. While in dealing with an `RDD` we relied on Python to convert types, here each column has a specified data type. This means that:\n",
    "\n",
    "a) we have to be a bit more careful about what we are doing and \n",
    "\n",
    "b) the execution engine can optimize our calculations because the data is no longer a black box. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and types for `DataFrame` API\n",
    "\n",
    "The next few cells are a bit lengthy and complicated so you can just treat them as black boxes for now. First, we just import some necessary libraries and classes, then a series of functions are defined and executed on the `data` to give us a `DataFrame` of hashtags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import Row, Window\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType, NullType, LongType, StructField, StructType, DateType, DataType, DateConverter, DatetimeConverter, TimestampType, BooleanType\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions to extract hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the date format to YYYY-MM-DD HH:MM:SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_date_string = func.udf(lambda date_string: \\\n",
    "                               datetime.date.strftime(datetime.datetime.strptime(date_string, \\\n",
    "                                                                                 '%a %b %d %H:%M:%S +0000 %Y'),\\\n",
    "                                                      '%Y-%m-%d %H:%M:%S'), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the date string to a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime_udf = func.udf(lambda date_string: datetime.strptime(date_string, '%a %b %d %H:%M:%S +0000 %Y'), DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the hashtags from a hashtag array and convert them all to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_text_udf = func.udf(lambda row: [r.text.lower() for r in row], returnType=ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tweet text string to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_udf_lower = func.udf(lambda row: row.lower(), returnType=StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_udf_split = func.udf(lambda row: row.split(), returnType=ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any of the given keywords is present in the tweet text word list. Returns a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# key_udf_key = func.udf(lambda row: key in row, returnType=BooleanType())\n",
    "key_udf_key = func.udf(lambda row: len([val for val in keys if val in row]) > 0, returnType=BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the year of creation of the tweet and the day of creation to a unique date ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_date = (lambda col: func.dayofyear(col) + func.year(col)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the functions to the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of partitions for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Npartitions = sc.defaultParallelism*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the hashtag dataframe with all the tweets with more than one hashtag, extract the hashtags and also extract the creation date and convert it to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[created_at: string, hashtags: array<struct<indices:array<bigint>,text:string>>, text: string, hash_text: array<string>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep the tweets with at least one hashtag\n",
    "hashtag_df = (data.select('created_at', 'entities.hashtags', 'text')\n",
    "                .filter(func.size('hashtags') > 1)\n",
    "                .withColumn('hash_text', hash_text_udf('hashtags'))\n",
    "                .withColumn('created_at', convert_date_string('created_at'))\n",
    "                .repartition(Npartitions))\n",
    "hashtag_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "|         created_at|            hashtags|                text|           hash_text|\n",
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "|2014-10-01 07:27:52|[[WrappedArray(64...|RT @2pmalways: RT...|   [trueswag, jun_k]|\n",
      "|2014-10-02 18:53:08|[[WrappedArray(40...|Que esa frase me ...|[guerreroencanalf...|\n",
      "|2014-10-03 05:01:07|[[WrappedArray(75...|New Warfare Has E...|[dalycity, bayvie...|\n",
      "|2014-10-04 00:15:00|[[WrappedArray(41...|RT @dafnehurley: ...|[rt, fav, dinámicas]|\n",
      "|2014-10-04 05:18:56|[[WrappedArray(99...|M16 Sentinel Mech...|[vape, vaporizer,...|\n",
      "|2014-10-04 21:39:47|[[WrappedArray(62...|秋のお出かけなら【登別温泉】へ行き...|    [北海道, 旅行, 秋, 紅葉]|\n",
      "|2014-10-05 17:20:46|[[WrappedArray(57...|RT @realmadrid: ....|[realmadridathlet...|\n",
      "|2014-10-06 10:47:52|[[WrappedArray(91...|RT @Asr3Follow01:...|[ريتويت, تابعني_ا...|\n",
      "|2014-10-07 14:32:00|[[WrappedArray(24...|@Marcelapq10 than...|  [following, dance]|\n",
      "|2014-10-08 00:28:58|[[WrappedArray(49...|I got a reward: L...|[ipad, gameinsigh...|\n",
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashtag_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tweet text to lowercase, split it into a word array and add this into a new column called test. Then check if any of the elements of keys is in this list and add this information in the column intext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "|         created_at|            hashtags|                text|           hash_text|                test|intext|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "|2014-10-01 07:27:52|[[WrappedArray(64...|RT @2pmalways: RT...|   [trueswag, jun_k]|[rt, @2pmalways:,...|  true|\n",
      "|2014-10-02 18:53:08|[[WrappedArray(40...|Que esa frase me ...|[guerreroencanalf...|[que, esa, frase,...| false|\n",
      "|2014-10-03 05:01:07|[[WrappedArray(75...|New Warfare Has E...|[dalycity, bayvie...|[new, warfare, ha...| false|\n",
      "|2014-10-04 00:15:00|[[WrappedArray(41...|RT @dafnehurley: ...|[rt, fav, dinámicas]|[rt, @dafnehurley...|  true|\n",
      "|2014-10-04 05:18:56|[[WrappedArray(99...|M16 Sentinel Mech...|[vape, vaporizer,...|[m16, sentinel, m...| false|\n",
      "|2014-10-04 21:39:47|[[WrappedArray(62...|秋のお出かけなら【登別温泉】へ行き...|    [北海道, 旅行, 秋, 紅葉]|[秋のお出かけなら【登別温泉】へ行...| false|\n",
      "|2014-10-05 17:20:46|[[WrappedArray(57...|RT @realmadrid: ....|[realmadridathlet...|[rt, @realmadrid:...|  true|\n",
      "|2014-10-06 10:47:52|[[WrappedArray(91...|RT @Asr3Follow01:...|[ريتويت, تابعني_ا...|[rt, @asr3follow0...|  true|\n",
      "|2014-10-07 14:32:00|[[WrappedArray(24...|@Marcelapq10 than...|  [following, dance]|[@marcelapq10, th...| false|\n",
      "|2014-10-08 00:28:58|[[WrappedArray(49...|I got a reward: L...|[ipad, gameinsigh...|[i, got, a, rewar...| false|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# key = 'rt'\n",
    "keys = ['@2pmalways:', 'rt']\n",
    "\n",
    "hashtag_df = hashtag_df.withColumn('test', key_udf_lower('text'))\n",
    "hashtag_df = hashtag_df.withColumn('test', key_udf_split('test'))\n",
    "hashtag_df = hashtag_df.withColumn('intext', key_udf_key('test'))\n",
    "hashtag_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the day ID of the tweet and add it to the column new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtag_df = hashtag_df.withColumn('new_date', new_date('created_at'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------+--------+\n",
      "|         created_at|            hashtags|                text|           hash_text|                test|intext|new_date|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------+--------+\n",
      "|2014-10-01 07:27:52|[[WrappedArray(64...|RT @2pmalways: RT...|   [trueswag, jun_k]|[rt, @2pmalways:,...|  true| 2014274|\n",
      "|2014-10-02 18:53:08|[[WrappedArray(40...|Que esa frase me ...|[guerreroencanalf...|[que, esa, frase,...| false| 2014275|\n",
      "|2014-10-03 05:01:07|[[WrappedArray(75...|New Warfare Has E...|[dalycity, bayvie...|[new, warfare, ha...| false| 2014276|\n",
      "|2014-10-04 00:15:00|[[WrappedArray(41...|RT @dafnehurley: ...|[rt, fav, dinámicas]|[rt, @dafnehurley...|  true| 2014277|\n",
      "|2014-10-04 05:18:56|[[WrappedArray(99...|M16 Sentinel Mech...|[vape, vaporizer,...|[m16, sentinel, m...| false| 2014277|\n",
      "|2014-10-04 21:39:47|[[WrappedArray(62...|秋のお出かけなら【登別温泉】へ行き...|    [北海道, 旅行, 秋, 紅葉]|[秋のお出かけなら【登別温泉】へ行...| false| 2014277|\n",
      "|2014-10-05 17:20:46|[[WrappedArray(57...|RT @realmadrid: ....|[realmadridathlet...|[rt, @realmadrid:...|  true| 2014278|\n",
      "|2014-10-06 10:47:52|[[WrappedArray(91...|RT @Asr3Follow01:...|[ريتويت, تابعني_ا...|[rt, @asr3follow0...|  true| 2014279|\n",
      "|2014-10-07 14:32:00|[[WrappedArray(24...|@Marcelapq10 than...|  [following, dance]|[@marcelapq10, th...| false| 2014280|\n",
      "|2014-10-08 00:28:58|[[WrappedArray(49...|I got a reward: L...|[ipad, gameinsigh...|[i, got, a, rewar...| false| 2014281|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------------+------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashtag_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Questions:\n",
    "* What does the text method do?\n",
    "* Why does the new_date function work even though it is not an UDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
